{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Imports & configuration\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "import openml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45f9eb",
   "metadata": {},
   "source": [
    "2.PROBLEM DEFINITION\n",
    "\n",
    "Goal:\n",
    "In this project we study how sensitive Linear Discriminant Analysis (LDA) is to noise and outliers, and propose a robust variant based on a robust covariance estimator. We evaluate both the standard and robust LDA on several datasets from the OpenML-CC18 curated classification benchmark.\n",
    "\n",
    "Research questions:\n",
    "\n",
    "How much does the performance of standard LDA degrade when we introduce outliers in the training data?\n",
    "\n",
    "Can a robust covariance-based variant of LDA reduce this degradation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84157674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>#instances</th>\n",
       "      <th>#attributes_total</th>\n",
       "      <th>#features</th>\n",
       "      <th>#classes</th>\n",
       "      <th>#numeric_features</th>\n",
       "      <th>#categorical_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dataset9.arff</td>\n",
       "      <td>Class</td>\n",
       "      <td>569</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset5.arff</td>\n",
       "      <td>Class</td>\n",
       "      <td>748</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset3.arff</td>\n",
       "      <td>class</td>\n",
       "      <td>768</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset2.arff</td>\n",
       "      <td>class</td>\n",
       "      <td>1000</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset12.arff</td>\n",
       "      <td>Class</td>\n",
       "      <td>1055</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset8.arff</td>\n",
       "      <td>Class</td>\n",
       "      <td>1941</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset7.arff</td>\n",
       "      <td>Class</td>\n",
       "      <td>5404</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename target  #instances  #attributes_total  #features  #classes  \\\n",
       "10   dataset9.arff  Class         569                 31         30         2   \n",
       "6    dataset5.arff  Class         748                  5          4         2   \n",
       "4    dataset3.arff  class         768                  9          8         2   \n",
       "3    dataset2.arff  class        1000                 21         20         2   \n",
       "2   dataset12.arff  Class        1055                 42         41         2   \n",
       "9    dataset8.arff  Class        1941                 34         33         2   \n",
       "8    dataset7.arff  Class        5404                  6          5         2   \n",
       "\n",
       "    #numeric_features  #categorical_features  \n",
       "10                 30                      0  \n",
       "6                   4                      0  \n",
       "4                   8                      0  \n",
       "3                   7                     13  \n",
       "2                  41                      0  \n",
       "9                  33                      0  \n",
       "8                   5                      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect datasets\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "DATA_DIR = \"datasets\"  # change if your folder has a different name\n",
    "\n",
    "def load_arff_to_df(path):\n",
    "    \"\"\"Load an ARFF file into a pandas DataFrame, decoding bytes if needed.\"\"\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Decode byte strings to normal strings (common in ARFF)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].apply(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    \n",
    "    return df, meta\n",
    "\n",
    "def summarize_dataset(path):\n",
    "    \"\"\"Return a dict with key info about one dataset.\"\"\"\n",
    "    df, meta = load_arff_to_df(path)\n",
    "    \n",
    "    n_instances = df.shape[0]\n",
    "    n_attributes = df.shape[1]\n",
    "    \n",
    "    # Assume last column is the target (typical for OpenML ARFF)\n",
    "    target_name = df.columns[-1]\n",
    "    y = df[target_name]\n",
    "    n_classes = y.nunique()\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = df.columns[:-1]\n",
    "    \n",
    "    # Count numeric vs categorical\n",
    "    n_numeric = 0\n",
    "    n_categorical = 0\n",
    "    for attr_name in feature_cols:\n",
    "        # Use meta to check nominal vs numeric\n",
    "        attr_type = meta[attr_name][0]  # e.g., 'numeric' or a list of categories\n",
    "        if isinstance(attr_type, str) and attr_type.lower() in [\"numeric\", \"real\", \"integer\"]:\n",
    "            n_numeric += 1\n",
    "        else:\n",
    "            n_categorical += 1\n",
    "    \n",
    "    return {\n",
    "        \"filename\": os.path.basename(path),\n",
    "        \"target\": target_name,\n",
    "        \"#instances\": n_instances,\n",
    "        \"#attributes_total\": n_attributes,\n",
    "        \"#features\": len(feature_cols),\n",
    "        \"#classes\": int(n_classes),\n",
    "        \"#numeric_features\": n_numeric,\n",
    "        \"#categorical_features\": n_categorical\n",
    "    }\n",
    "\n",
    "# Scan all .arff files\n",
    "arff_files = glob.glob(os.path.join(DATA_DIR, \"*.arff\"))\n",
    "\n",
    "summaries = [summarize_dataset(p) for p in arff_files]\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "\n",
    "# Sort to make it easier to inspect\n",
    "df_summary = df_summary.sort_values(by=\"#instances\")\n",
    "\n",
    "df_suitable = df_summary[\n",
    "    (df_summary[\"#instances\"] >= 200) &\n",
    "    (df_summary[\"#instances\"] <= 20000) &\n",
    "    (df_summary[\"#features\"] <= 100) &\n",
    "    (df_summary[\"#classes\"] >= 2)\n",
    "].copy()\n",
    "\n",
    "df_suitable = df_suitable[df_suitable[\"#numeric_features\"] >= 1]\n",
    "df_suitable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b9786",
   "metadata": {},
   "source": [
    "We downloaded 11 datasets from Open ML and inspected them to check if they were suitable for LDA. We applied 2 main filters for this determination:\n",
    "\n",
    "1. Checked that the dataset wasn't too small or too big to keep runtime low\n",
    "2. Be a classification dataset (target is categorical, meaning it has 2 or more classes)\n",
    "3. That they had at least 1 numerical feature\n",
    "4. Have a reasonal number of features (<100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54fff3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset10.arff (556, 6) ['1' '2']\n",
      "dataset6.arff (601, 6) ['2' '1']\n",
      "dataset4.arff (958, 9) ['positive' 'negative']\n",
      "dataset3.arff (768, 8) ['tested_positive' 'tested_negative']\n",
      "dataset2.arff (1000, 20) ['good' 'bad']\n",
      "dataset9.arff (569, 30) ['2' '1']\n",
      "dataset8.arff (1941, 33) ['1' '2']\n"
     ]
    }
   ],
   "source": [
    "# Loading chosen datasets\n",
    "from scipy.io import arff\n",
    "\n",
    "DATA_DIR = \"datasets\"\n",
    "\n",
    "selected_files = [\n",
    "    \"dataset10.arff\",\n",
    "    \"dataset6.arff\",\n",
    "    \"dataset4.arff\",\n",
    "    \"dataset3.arff\",\n",
    "    \"dataset2.arff\",\n",
    "    \"dataset9.arff\",\n",
    "    \"dataset8.arff\"\n",
    "]\n",
    "\n",
    "def load_dataset(fname):\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # decode byte strings\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].apply(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    \n",
    "    target = df.columns[-1]  # typically last column\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    return X, y, meta\n",
    "\n",
    "# Test load\n",
    "for f in selected_files:\n",
    "    X, y, meta = load_dataset(f)\n",
    "    print(f, X.shape, y.unique()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb2eb1",
   "metadata": {},
   "source": [
    "PREPROCESSING STEP: \n",
    "\n",
    "LDA (Linear Discriminant Analysis) is very sensitive to:\n",
    "\n",
    "- feature scale\n",
    "\n",
    "- presence of categorical variables\n",
    "\n",
    "- covariance estimation problems\n",
    "\n",
    "- numerical instability\n",
    "\n",
    "So preprocessing ensures the data is in a numerically stable, usable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f1bf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "def make_preprocessor(X):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline that:\n",
    "      - Standardizes numeric features\n",
    "      - One-hot encodes categorical features\n",
    "\n",
    "    Returns:\n",
    "      preprocessor: a ColumnTransformer object\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Detect numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # 2. Categorical columns = everything else\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    print(\"Numeric columns:\", numeric_cols)\n",
    "    print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "    # 3. Define transformations\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "    # 4. Build ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30']\n",
      "Categorical columns: []\n",
      "Original shape: (569, 30)\n",
      "Processed shape: (569, 30)\n",
      "Data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#TEST PREPROCESSOR on a dataset\n",
    "X, y, meta = load_dataset(\"dataset9.arff\")\n",
    "\n",
    "preprocessor = make_preprocessor(X)\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Processed shape:\", X_proc.shape)\n",
    "print(\"Data type:\", type(X_proc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40daffba",
   "metadata": {},
   "source": [
    "APPLYING BASELINE LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebe550",
   "metadata": {},
   "source": [
    "The outlier injection function below does the following: \n",
    "Selects 10% of samples randomly (outlier_fraction)\n",
    "\n",
    "Adds strong noise (outlier_strength=5.0)\n",
    "\n",
    "Makes those samples extreme → outliers\n",
    "\n",
    "Perfect for showing LDA’s sensitivity to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96d05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER INJECTION FUNCTION\n",
    "def inject_outliers(X, y, outlier_fraction=0.1, outlier_strength=5.0):\n",
    "    \"\"\"\n",
    "    Injects synthetic outliers by adding strong Gaussian noise to a fraction of the samples.\n",
    "    Only adds noise to X (not y).\n",
    "    \"\"\"\n",
    "    X_noisy = X.copy()\n",
    "    n_samples = X_noisy.shape[0]\n",
    "    n_outliers = int(outlier_fraction * n_samples)\n",
    "\n",
    "    if n_outliers == 0:\n",
    "        return X_noisy, y\n",
    "\n",
    "    outlier_idx = np.random.choice(n_samples, size=n_outliers, replace=False)\n",
    "    noise = np.random.normal(loc=0.0, scale=outlier_strength, size=X_noisy[outlier_idx].shape)\n",
    "\n",
    "    X_noisy[outlier_idx] = X_noisy[outlier_idx] + noise\n",
    "\n",
    "    return X_noisy, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e427538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION FUNCTION FOR LDA\n",
    "def evaluate_lda_on_dataset(X, y, preprocessor, inject_noise=False, \n",
    "                            outlier_fraction=0.1, outlier_strength=5.0,\n",
    "                            n_splits=5):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        # Split\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Preprocess\n",
    "        X_train_pp = preprocessor.fit_transform(X_train)\n",
    "        X_test_pp  = preprocessor.transform(X_test)\n",
    "\n",
    "        # Convert to dense array for LDA\n",
    "        if hasattr(X_train_pp, \"toarray\"):\n",
    "            X_train_pp = X_train_pp.toarray()\n",
    "        if hasattr(X_test_pp, \"toarray\"):\n",
    "            X_test_pp = X_test_pp.toarray()\n",
    "\n",
    "        # Inject outliers into training data only\n",
    "        if inject_noise:\n",
    "            X_train_pp, y_train = inject_outliers(\n",
    "                X_train_pp, y_train,\n",
    "                outlier_fraction=outlier_fraction,\n",
    "                outlier_strength=outlier_strength\n",
    "            )\n",
    "\n",
    "        # Train baseline LDA\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train_pp, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = lda.predict(X_test_pp)\n",
    "\n",
    "        # Store metrics\n",
    "        acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    return np.mean(acc_scores), np.mean(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d394a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dataset10.arff ...\n",
      "Numeric columns: []\n",
      "Categorical columns: ['class', 'attr1', 'attr2', 'attr3', 'attr4', 'attr5']\n",
      "Evaluating dataset6.arff ...\n",
      "Numeric columns: []\n",
      "Categorical columns: ['class', 'attr1', 'attr2', 'attr3', 'attr4', 'attr5']\n",
      "Evaluating dataset4.arff ...\n",
      "Numeric columns: []\n",
      "Categorical columns: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']\n",
      "Evaluating dataset3.arff ...\n",
      "Numeric columns: ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age']\n",
      "Categorical columns: []\n",
      "Evaluating dataset2.arff ...\n",
      "Numeric columns: ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
      "Categorical columns: ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n",
      "Evaluating dataset9.arff ...\n",
      "Numeric columns: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30']\n",
      "Categorical columns: []\n",
      "Evaluating dataset8.arff ...\n",
      "Numeric columns: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33']\n",
      "Categorical columns: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>acc_clean</th>\n",
       "      <th>f1_clean</th>\n",
       "      <th>acc_noisy</th>\n",
       "      <th>f1_noisy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset10.arff</td>\n",
       "      <td>0.431676</td>\n",
       "      <td>0.420822</td>\n",
       "      <td>0.485682</td>\n",
       "      <td>0.474668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset6.arff</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.411309</td>\n",
       "      <td>0.477479</td>\n",
       "      <td>0.476785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset4.arff</td>\n",
       "      <td>0.983295</td>\n",
       "      <td>0.981317</td>\n",
       "      <td>0.660755</td>\n",
       "      <td>0.416931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset3.arff</td>\n",
       "      <td>0.765589</td>\n",
       "      <td>0.726942</td>\n",
       "      <td>0.729174</td>\n",
       "      <td>0.633672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset2.arff</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.676888</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.515073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset9.arff</td>\n",
       "      <td>0.956094</td>\n",
       "      <td>0.951662</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.945892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset8.arff</td>\n",
       "      <td>0.766103</td>\n",
       "      <td>0.719429</td>\n",
       "      <td>0.823309</td>\n",
       "      <td>0.767268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset  acc_clean  f1_clean  acc_noisy  f1_noisy\n",
       "0  dataset10.arff   0.431676  0.420822   0.485682  0.474668\n",
       "1   dataset6.arff   0.412700  0.411309   0.477479  0.476785\n",
       "2   dataset4.arff   0.983295  0.981317   0.660755  0.416931\n",
       "3   dataset3.arff   0.765589  0.726942   0.729174  0.633672\n",
       "4   dataset2.arff   0.745000  0.676888   0.717000  0.515073\n",
       "5   dataset9.arff   0.956094  0.951662   0.950800  0.945892\n",
       "6   dataset8.arff   0.766103  0.719429   0.823309  0.767268"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUNNING LDA ON CHOSEN DATASETS\n",
    "baseline_results = []\n",
    "\n",
    "for fname in selected_files:\n",
    "    print(f\"Evaluating {fname} ...\")\n",
    "\n",
    "    X, y, meta = load_dataset(fname)\n",
    "    preprocessor = make_preprocessor(X)\n",
    "\n",
    "    # Clean\n",
    "    acc_clean, f1_clean = evaluate_lda_on_dataset(\n",
    "        X, y, preprocessor,\n",
    "        inject_noise=False\n",
    "    )\n",
    "\n",
    "    # Noisy\n",
    "    acc_noisy, f1_noisy = evaluate_lda_on_dataset(\n",
    "        X, y, preprocessor,\n",
    "        inject_noise=True,\n",
    "        outlier_fraction=0.1,\n",
    "        outlier_strength=5.0\n",
    "    )\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"dataset\": fname,\n",
    "        \"acc_clean\": acc_clean,\n",
    "        \"f1_clean\": f1_clean,\n",
    "        \"acc_noisy\": acc_noisy,\n",
    "        \"f1_noisy\": f1_noisy\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df_baseline = pd.DataFrame(baseline_results)\n",
    "df_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe177179",
   "metadata": {},
   "source": [
    "INTERPRETATION OF BASELINE LDA RESULTS:\n",
    "\n",
    "The baseline LDA algorithm showed varying sensitivity to noise and outliers across datasets.\n",
    "\n",
    "In some datasets (e.g., dataset4.arff), performance decreased drastically when outliers were injected, demonstrating LDA’s reliance on stable covariance and mean estimates.\n",
    "\n",
    "In other datasets (e.g., dataset9.arff and dataset8.arff), performance was relatively stable, suggesting that the data distribution was either robust to perturbations or dominated by strong signals.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
