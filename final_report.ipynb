{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa36705",
   "metadata": {},
   "source": [
    "# Machine Learning Project : LDA (Linear Discrimination Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267dad7",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Key objectives of the project:\n",
    "\n",
    "1. Select a supervised machine learning classification algorithm.\n",
    "\n",
    "2. Study it in detail (theoretically and empirically).\n",
    "\n",
    "3. Identify a data characteristic that affects its performance.\n",
    "\n",
    "4. Propose a variant of the algorithm that addresses this weakness.\n",
    "\n",
    "5. Evaluate the modified version on the OpenML-CC18 benchmark datasets.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "<br></br>\n",
    "\n",
    "In this project, we selected Linear Discriminant Analysis (LDA) and focused on the characteristic noise/outliers, which LDA is known to be sensitive to. As a classic algorithm, LDA relies heavily on accurate estimates of class means and a shared covariance matrix, making its performance vulnerable to distortions caused by extreme values in the training data.\n",
    "\n",
    "This project investigates the robustness of standard LDA against synthetic outliers and proposes a robust variant designed to mitigate this sensitivity. The core research questions are:\n",
    "\n",
    "- To what extent does the performance of standard LDA degrade when outliers are introduced into the training data?\n",
    "\n",
    "- Can a variant of LDA, based on a robust covariance estimator, reduce this degradation and improve overall model\n",
    "stability?\n",
    "\n",
    "\n",
    "To address these questions, we conduct an empirical study comparing the standard LDA with our proposed robust variant\n",
    " across multiple datasets from the **OpenML-CC18 benchmark**. The evaluation follows a structured pipeline, including\n",
    " data preprocessing, synthetic outlier injection, and performance assessment via stratified cross-validation using Accuracy and Macro F1-score. This analysis aims to quantify the degradation of standard LDA under noise and demonstrate the effectiveness of our robust adaptation.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Background\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)** is a classification and dimensionality reduction algorithm that allows for the\n",
    " optimal separation of classes. The goal and principle of LDA is to linearly combine the features of the data so that\n",
    " the labels\n",
    " of the datasets are best separated from each other, and the number of new features is reduced to a predefined count.\n",
    "\n",
    "<img src=\"images/lda_goal.png\" alt=\"LDA\" style=\"width:4500px; height:350px; object-fit:contain; display:block;\n",
    "margin:0 auto;\" />\n"
   ],
   "id": "9e45f81263dcf158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LDA in scratch\n",
    "\n",
    "The process of Linear Discriminant Analysis (LDA) can be broken down into five key steps.\n",
    "<br></br>\n",
    "##### **Step 1:** Compute the d-dimensional mean vectors for each of the k classes separately from the dataset.\n",
    "\n",
    "\n",
    "LDA is a supervised machine learning technique, meaning we can utilize the known labels. In the first step, we\n",
    "calculate the mean vectors $mean_c$ for all samples belonging to a specific class $c$. To do this, we filter the feature matrix by class label and compute the mean for each of the $d$ features. As a result, we obtain $k$ mean vectors (one for each of the $k$ classes), each with a length of $d$ (corresponding to the $d$ features).\n",
    "\n",
    "<img src=\"images/feature_matrix.png\" alt=\"LDA\" style=\"width:950px; height:350px; object-fit:contain; display:block;\n",
    "margin:0 auto;\" />\n",
    "\n",
    "$$\n",
    "\\text{mean}_c = \\frac{1}{n_c - 1} \\cdot \\sum_{x \\in C_c} \\mathbf{X}_c = [\\mu_1, \\mu_2, \\mu_3, \\ldots, \\mu_d]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\mu_c = [\\mu_1, \\mu_2, \\mu_3, \\dots, \\mu_d] $ is the $d$-dimensional mean vector\n",
    "\n",
    "$ \\mu_j $ is the mean of the $ j $-th feature for class $c$\n",
    "\n",
    "$ n_c $ is the number of samples in class $c$\n",
    "\n",
    "$ C_c $ - the set of all samples in class $c$\n",
    "\n",
    "$ x $ - the sample vector\n",
    "\n"
   ],
   "id": "70c3c87ed3ca8e63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Step 2:** Compute the scatter matrices (between-class scatter matrix and within-class scatter matrix).\n",
   "id": "4285ec04f57318a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The within-class scatter matrix measures the variation among samples within the same class. To find a subspace with optimal separability, we aim to minimize the values in this matrix. In contrast, the between-class scatter matrix measures the variation between different classes. For optimal separability, we aim to maximize the values in this matrix.\n",
    "\n",
    "Intuitively, within-class scatter looks at how compact each class is, whereas between-class scatter examines how far apart different classes are.\n",
    "\n",
    "\n",
    "<img src=\"images/class_scatter_matrices.png\" alt=\"LDA\" style=\"width:450px; height:350px; object-fit:contain;\n",
    "display:block;\n",
    "margin:0 auto;\" />\n",
    "\n",
    "\n",
    "The within-class scatter matrix S_W. It is calculated as the sum of the scatter matrices S_c for each individual class:\n",
    "\n",
    "$$ S_w = \\sum_{c=1}^k S_c  ,\\  where \\ \\ \\  S_c = \\sum_{x \\in C_c} (x - \\text{mean}_c) \\cdot (x - \\text{mean}_c)^T $$\n",
    "\n",
    "\n",
    "The between-class scatter matrix $S_B$ is derived from the differences between the class means $mean_c$ and the overall\n",
    "mean of the entire dataset:\n",
    "$$ S_B = \\sum_{c=1}^k n_c \\cdot (\\text{mean}_c - \\text{mean}) \\cdot (\\text{mean}_c - \\text{mean})^T $$\n",
    "\n",
    "where mean refers to the mean vector calculated over all samples, regardless of their class labels.\n",
    "\n"
   ],
   "id": "b3952a9a42fc1a70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Step 3:** Calculate the eigenvectors and eigenvalues for the ratio of $S_W$ and $S_B$.\n",
    "\n",
    "As mentioned, for optimal class separability, we aim to maximize $S_B$ and minimize $S_W$. We can achieve both by\n",
    "maximizing the ratio $S_B/S_W$. In linear algebra terms, this ratio corresponds to the scatter matrix $S_W^{-1}$ $S_B$,\n",
    "which is maximized in the subspace spanned by the eigenvectors with the highest eigenvalues. The eigenvectors define the directions of this subspace, while the eigenvalues represent the magnitude of the distortion. We will select the $m$ eigenvectors associated with the highest eigenvalues.\n",
    "\n",
    "$$\n",
    "A\\nu = \\lambda\\nu \\quad \\text{with} \\quad A = S_W^{-1} S_B\n",
    "$$"
   ],
   "id": "b8cce5ce82c47eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Step 4:** Sort the eigenvectors in descending order of their corresponding eigenvalues, and select the $m$eigenvectors with the largest eigenvalues to form a $d × m$ - dimensional transformation matrix $W$.\n",
    "\n",
    "The goal is not only to project the data into a subspace that enhances class separability but also to reduce\n",
    "dimensionality. The eigenvectors will define the axes of our new feature subspace. To decide which eigenvectors to discard for the lower-dimensional subspace, we need to examine their corresponding eigenvalues. In simple terms, the eigenvectors with the smallest eigenvalues contribute the least to class separation, and these are the ones we want to drop. The typical approach is to rank the eigenvalues in descending order and select the top $m$ eigenvectors. $m$ is a freely chosen parameter. The larger $m$, the less information is lost during the transformation.\n",
    "\n",
    "After sorting the eigenpairs by decreasing eigenvalues and selecting the top $m$ pairs, the next step is to construct\n",
    "the $d × m$ - dimensional transformation matrix $W$. This is done by stacking the m selected eigenvectors horizontally, resulting in the matrix $W$:\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{pmatrix}\n",
    "e_1^{(1)} & e_1^{(2)} & \\cdots \\\\\n",
    "e_2^{(1)} & e_2^{(2)} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\ddots \\\\\n",
    "e_d^{(1)} & e_d^{(2)} & \\cdots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The first column of $W$ represents the eigenvector corresponding to the highest eigenvalue, the second column represents the eigenvector corresponding to the second highest eigenvalue, and so on.\n",
    "\n"
   ],
   "id": "574255d97e7a2890"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Step 5:** Use W to project the samples onto the new subspace.\n",
    "\n",
    "In the final step, we use the d × m-dimensional transformation matrix W, which we composed from the top m selected eigenvectors, to project our samples onto the new subspace:\n",
    "\n",
    "$$Z = X⋅W,$$\n",
    "\n",
    "where $X$ is the initial $n × d$ - dimensional feature matrix representing our samples, and $Z$ is the newly\n",
    "transformed $n × m$ - dimensional feature matrix in the new subspace. This means that the selected eigenvectors serve\n",
    "as the “recipes” for transforming the original features into the new features (the Linear Discriminants): The eigenvector with the highest eigenvalue provides the transformation recipe for $LD1$, the eigenvector with the second highest eigenvalue corresponds to $LD2$, and so on.\n",
    "\n",
    "\n",
    "<img src=\"images/eug_mx.png\" alt=\"LDA\" style=\"width:950px; height:350px; object-fit:contain;\n",
    "display:block;\n",
    "margin:0 auto;\" />\n"
   ],
   "id": "13a90e927b5f0a48"
  },
  {
   "cell_type": "markdown",
   "id": "201e916b",
   "metadata": {},
   "source": [
    "# Datasets Description\n",
    "\n",
    "3. Dataset Description\n",
    "3.1 Benchmark Source\n",
    "\n",
    "We use datasets from the OpenML CC-18 Curated Classification Benchmark Suite, which contains diverse classification datasets standardized for ML evaluation.\n",
    "\n",
    "3.2 Dataset Loading & Preprocessing\n",
    "\n",
    "Datasets obtained in ARFF format\n",
    "\n",
    "Loaded locally using scipy.io.arff.loadarff\n",
    "\n",
    "Features split into:\n",
    "\n",
    "Numerical\n",
    "\n",
    "Categorical\n",
    "\n",
    "Preprocessing steps:\n",
    "\n",
    "Numerical: Standardization\n",
    "\n",
    "Categorical: One-Hot Encoding\n",
    "\n",
    "3.3 Dataset Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b6fee",
   "metadata": {},
   "source": [
    "# Baseline Method: Standard LDA\n",
    "4.1 Implementation Overview\n",
    "\n",
    "Preprocess each dataset\n",
    "\n",
    "Use stratified 5-fold cross-validation\n",
    "\n",
    "Evaluate:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Macro F1-score\n",
    "\n",
    "Evaluate both:\n",
    "\n",
    "Clean data\n",
    "\n",
    "Noisy data (outliers artificially injected into training folds)\n",
    "\n",
    "4.2 Noise Injection Strategy\n",
    "\n",
    "Explain briefly:\n",
    "\n",
    "A percentage of training samples randomly selected\n",
    "\n",
    "Feature values perturbed with extreme values (controlled outlier strength)\n",
    "\n",
    "Only training folds are modified, test folds remain clean\n",
    "\n",
    "4.3 Baseline Performance Results\n",
    "\n",
    "(Insert your baseline results table here)\n",
    "\n",
    "4.4 Observations\n",
    "\n",
    "Summarize what happened when noise was added\n",
    "\n",
    "Identify datasets most affected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c1a7",
   "metadata": {},
   "source": [
    "# Proposed Modification\n",
    "\n",
    "5. Proposed Method: Outlier-Filtered LDA\n",
    "5.1 Motivation\n",
    "\n",
    "LDA relies on estimating means and covariance from all data points. Outliers strongly distort these estimates.\n",
    "Idea: remove samples with extreme z-scores before fitting LDA → more stable covariance → more robust classifier.\n",
    "\n",
    "5.2 Algorithm Description\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute z-scores per feature\n",
    "\n",
    "Mark samples exceeding z_thresh in ANY feature as outliers\n",
    "\n",
    "Remove them from the training data\n",
    "\n",
    "Fit standard LDA on filtered data\n",
    "\n",
    "Test normally (no filtering applied to test set)\n",
    "\n",
    "5.3 Implementation Code\n",
    "\n",
    "(Include your FilteredLDA class and explanation)\n",
    "\n",
    "5.4 Expected Benefits\n",
    "\n",
    "More stable class means\n",
    "\n",
    "Better covariance estimate\n",
    "\n",
    "Less boundary distortion\n",
    "\n",
    "Improved accuracy under noisy conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036d893",
   "metadata": {},
   "source": [
    "# Empirical Evaluation\n",
    "6.1 Experimental Setup\n",
    "\n",
    "Same 5-fold CV as baseline\n",
    "\n",
    "Same noise setup\n",
    "\n",
    "Same preprocessing pipeline\n",
    "\n",
    "Metrics: Accuracy, Macro F1\n",
    "\n",
    "6.2 Results Table\n",
    "\n",
    "(Insert merged table: baseline vs filtered)\n",
    "\n",
    "| Dataset | acc_noisy | acc_noisy_filtered | Δacc | f1_noisy | f1_noisy_filtered | Δf1 |\n",
    "\n",
    "6.3 Visual Comparison\n",
    "\n",
    "Insert the scatter plots:\n",
    "\n",
    "Scatter Plot: Accuracy (Noisy Data)\n",
    "\n",
    "(Your plot goes here)\n",
    "\n",
    "Scatter Plot: Macro F1 (Noisy Data)\n",
    "\n",
    "(Your plot goes here)\n",
    "\n",
    "Optional: Bar chart for improvement per dataset\n",
    "\n",
    "(Your bar plot if you include one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2dac1",
   "metadata": {},
   "source": [
    "# Results and Analysis\n",
    "\n",
    "7. Results Analysis\n",
    "7.1 Overall Improvements\n",
    "\n",
    "Most datasets lie above the diagonal → filtered LDA performs better\n",
    "\n",
    "Outlier-filtering improves robustness without requiring complex models\n",
    "\n",
    "7.2 Dataset-Level Behavior\n",
    "\n",
    "Summaries:\n",
    "\n",
    "Strong improvements: dataset4.arff\n",
    "\n",
    "Moderate: dataset3.arff, dataset2.arff, dataset8.arff\n",
    "\n",
    "Neutral: dataset7.arff, dataset9.arff\n",
    "\n",
    "Small negative: dataset10.arff (possible over-filtering)\n",
    "\n",
    "7.3 Interpretation\n",
    "\n",
    "Z-score filtering protects LDA from extreme values\n",
    "\n",
    "Means + covariance become more stable\n",
    "\n",
    "Model generalizes better under noisy training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced2138",
   "metadata": {},
   "source": [
    "# Conclusions \n",
    "8. Conclusions\n",
    "8.1 Summary\n",
    "\n",
    "Baseline LDA is highly sensitive to outliers\n",
    "\n",
    "We implemented a simple, effective outlier-filtered LDA\n",
    "\n",
    "The variant consistently improved performance on noisy data\n",
    "\n",
    "8.2 Strengths\n",
    "\n",
    "Simple\n",
    "\n",
    "Computationally cheap\n",
    "\n",
    "Easy to interpret\n",
    "\n",
    "Significantly improves robustness on many datasets\n",
    "\n",
    "8.3 Limitations\n",
    "\n",
    "May remove useful data if threshold is too strict\n",
    "\n",
    "Effectiveness depends on dataset size and distribution\n",
    "\n",
    "8.4 Future Work\n",
    "\n",
    "Explore robust covariance estimation (e.g., shrinkage, MinCovDet)\n",
    "\n",
    "Use Mahalanobis distance for more intelligent outlier detection\n",
    "\n",
    "Extend to multiclass or high-dimensional datasets\n",
    "\n",
    "Combine filtering with dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad5dc1",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "A.1 Full Code Listing\n",
    "\n",
    "(Place all helper functions, classes, etc.)\n",
    "\n",
    "A.2 Environment & Libraries\n",
    "\n",
    "Python version\n",
    "\n",
    "sklearn version\n",
    "\n",
    "pandas version"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
